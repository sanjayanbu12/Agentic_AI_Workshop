{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ca2756efb594899b22bc3f30a4ebbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b33b09453ddb47be92a3f1259670e1cd",
              "IPY_MODEL_41b2b0d4f85942f8a5df585372828437",
              "IPY_MODEL_c2cc0472befe4a6194bb7ecded8f86ce"
            ],
            "layout": "IPY_MODEL_9c8f8486497746b19bab28b9be40de28"
          }
        },
        "b33b09453ddb47be92a3f1259670e1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3734c6598fd445c9875120e0a1d434d1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f3f74330eed45b9816312ae82b4e3e3",
            "value": "Batches:â€‡100%"
          }
        },
        "41b2b0d4f85942f8a5df585372828437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca7b145a69754a15949f68b20911e67b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_053bedc6a395443882fd27cf975b7d1d",
            "value": 1
          }
        },
        "c2cc0472befe4a6194bb7ecded8f86ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f82a27d9104048b09c584ca08e472b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e0a356704f64442c87dc40989176824d",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡6.31it/s]"
          }
        },
        "9c8f8486497746b19bab28b9be40de28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3734c6598fd445c9875120e0a1d434d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f3f74330eed45b9816312ae82b4e3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca7b145a69754a15949f68b20911e67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "053bedc6a395443882fd27cf975b7d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72f82a27d9104048b09c584ca08e472b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0a356704f64442c87dc40989176824d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "124d30b57cc24b539389ddb9d88a8ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f2f47ebb5bc403c9d47970db12d5006",
              "IPY_MODEL_8263ffcb89d5429fa4aea54e09a667fc",
              "IPY_MODEL_7783212b9a1d492e9d5eca44fa73e580"
            ],
            "layout": "IPY_MODEL_7c1aa37406f340a483eb8688df1175e3"
          }
        },
        "6f2f47ebb5bc403c9d47970db12d5006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0a8cf907b34073a265c06c0061fdd2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a4fb31be67b643c9b875d3c8375357c5",
            "value": "Batches:â€‡100%"
          }
        },
        "8263ffcb89d5429fa4aea54e09a667fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08d7216aa55943dc806b2717f64a24a4",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c440cffc81cc4cc0bb4a498e8b848a7e",
            "value": 7
          }
        },
        "7783212b9a1d492e9d5eca44fa73e580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689f4bb0f17044ca8e8bce7c636fe0cc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ec839ca3980a40b48abd5b8737ab3021",
            "value": "â€‡7/7â€‡[00:26&lt;00:00,â€‡â€‡3.05s/it]"
          }
        },
        "7c1aa37406f340a483eb8688df1175e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0a8cf907b34073a265c06c0061fdd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4fb31be67b643c9b875d3c8375357c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08d7216aa55943dc806b2717f64a24a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c440cffc81cc4cc0bb4a498e8b848a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "689f4bb0f17044ca8e8bce7c636fe0cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec839ca3980a40b48abd5b8737ab3021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AdgpdTghBQa",
        "outputId": "4529d538-190f-4762-a846-258d332863ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All packages installed successfully!\n",
            "ðŸ“‹ Installed: transformers, torch, sentence-transformers, faiss, PyPDF2, etc.\n",
            "ðŸš€ Ready to proceed to Cell 2!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install Required Packages\n",
        "!pip install -q transformers torch sentence-transformers faiss-cpu PyPDF2 langchain tiktoken datasets accelerate numpy pandas scikit-learn ipywidgets\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")\n",
        "print(\"ðŸ“‹ Installed: transformers, torch, sentence-transformers, faiss, PyPDF2, etc.\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 2!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoModel,\n",
        "    pipeline,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check system capabilities\n",
        "print(f\"ðŸ“± System Information:\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"   Using device: {device}\")\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 3!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brzACdrChL4l",
        "outputId": "4cd5c8cb-96b3-4c1b-9dbd-a4b34ddf2713"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“± System Information:\n",
            "   PyTorch version: 2.6.0+cu124\n",
            "   CUDA available: False\n",
            "   Using device: cpu\n",
            "âœ… All libraries imported successfully!\n",
            "ðŸš€ Ready to proceed to Cell 3!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    \"\"\"Handles loading PDF/TXT files and breaking them into chunks\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 300, overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "        print(f\"ðŸ“„ DocumentProcessor ready: {chunk_size} words per chunk\")\n",
        "\n",
        "    def load_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading PDF {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def load_text_file(self, file_path: str) -> str:\n",
        "        \"\"\"Load text from .txt file\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading text file {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Remove extra whitespace and special characters\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:()\\-\\[\\]{}\"\\']', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def chunk_text(self, text: str, source: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        sentences = re.split(r'[.!?]+', text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if not sentence:\n",
        "                continue\n",
        "\n",
        "            sentence_words = len(sentence.split())\n",
        "\n",
        "            if current_length + sentence_words <= self.chunk_size:\n",
        "                current_chunk += sentence + \". \"\n",
        "                current_length += sentence_words\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append({\n",
        "                        'text': current_chunk.strip(),\n",
        "                        'source': source,\n",
        "                        'chunk_id': len(chunks),\n",
        "                        'word_count': current_length\n",
        "                    })\n",
        "\n",
        "                current_chunk = sentence + \". \"\n",
        "                current_length = sentence_words\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append({\n",
        "                'text': current_chunk.strip(),\n",
        "                'source': source,\n",
        "                'chunk_id': len(chunks),\n",
        "                'word_count': current_length\n",
        "            })\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_documents(self, file_paths: List[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process multiple documents into chunks\"\"\"\n",
        "        print(f\"ðŸ“š Processing {len(file_paths)} documents...\")\n",
        "\n",
        "        all_chunks = []\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            print(f\"ðŸ“„ Processing: {os.path.basename(file_path)}\")\n",
        "\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                text = self.load_pdf(file_path)\n",
        "            elif file_path.lower().endswith('.txt'):\n",
        "                text = self.load_text_file(file_path)\n",
        "            else:\n",
        "                print(f\"âš ï¸ Unsupported file type: {file_path}\")\n",
        "                continue\n",
        "\n",
        "            if not text:\n",
        "                continue\n",
        "\n",
        "            clean_text = self.clean_text(text)\n",
        "            source_name = os.path.basename(file_path)\n",
        "            chunks = self.chunk_text(clean_text, source_name)\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "            print(f\"   âœ… Created {len(chunks)} chunks\")\n",
        "\n",
        "        print(f\"ðŸŽ‰ Total chunks: {len(all_chunks)}\")\n",
        "        return all_chunks\n",
        "\n",
        "print(\"âœ… DocumentProcessor ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 4!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b2W9_udhr2n",
        "outputId": "b43349b3-d7f5-42dc-c9e4-69d0b3daf5f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DocumentProcessor ready!\n",
            "ðŸš€ Ready to proceed to Cell 4!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentRetriever:\n",
        "    \"\"\"Finds relevant document chunks for questions using AI embeddings\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.model_name = model_name\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "        print(f\"ðŸ” DocumentRetriever ready with {model_name}\")\n",
        "\n",
        "    def build_index(self, documents: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"Build searchable index from documents\"\"\"\n",
        "        print(f\"ðŸ—ï¸ Building search index for {len(documents)} chunks...\")\n",
        "\n",
        "        self.documents = documents\n",
        "        texts = [doc['text'] for doc in documents]\n",
        "\n",
        "        # Convert text to AI embeddings (numbers)\n",
        "        self.embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # Build fast search index\n",
        "        dimension = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        # Normalize for better similarity search\n",
        "        faiss.normalize_L2(self.embeddings)\n",
        "        self.index.add(self.embeddings.astype('float32'))\n",
        "\n",
        "        print(f\"âœ… Search index ready! Dimension: {dimension}\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Find most relevant document chunks for a question\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call build_index() first.\")\n",
        "\n",
        "        # Convert question to embedding\n",
        "        query_embedding = self.model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search for similar chunks\n",
        "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
        "\n",
        "        # Return results with similarity scores\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx != -1:\n",
        "                doc = self.documents[idx].copy()\n",
        "                doc['similarity_score'] = float(score)\n",
        "                doc['rank'] = i + 1\n",
        "                results.append(doc)\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"âœ… DocumentRetriever ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 5!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGI4RVtMh8-3",
        "outputId": "32bbaf92-b453-4893-e323-8f8081a03823"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… DocumentRetriever ready!\n",
            "ðŸš€ Ready to proceed to Cell 5!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerGenerator:\n",
        "    \"\"\"Generates intelligent answers using retrieved context\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"google/flan-t5-base\"):\n",
        "        self.model_name = model_name\n",
        "        print(f\"ðŸ¤– Loading answer generation model: {model_name}\")\n",
        "\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.model.to(device)\n",
        "\n",
        "        # Backup QA pipeline\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"question-answering\",\n",
        "            model=\"distilbert-base-cased-distilled-squad\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Answer generator ready!\")\n",
        "\n",
        "    def format_context(self, retrieved_docs: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format retrieved documents into context\"\"\"\n",
        "        context_parts = []\n",
        "        for i, doc in enumerate(retrieved_docs):\n",
        "            source = doc.get('source', 'Unknown')\n",
        "            text = doc['text']\n",
        "            context_parts.append(f\"[Document {i+1} - {source}]: {text}\")\n",
        "        return \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    def generate_answer_t5(self, question: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using T5 model\"\"\"\n",
        "        try:\n",
        "            # Format input for T5\n",
        "            input_text = f\"Answer the question based on the context.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                max_length=1024,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            # Generate answer\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=512,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            # Decode answer\n",
        "            answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            return {\n",
        "                'answer': answer.strip(),\n",
        "                'confidence': 0.8,  # Default confidence for T5\n",
        "                'method': 't5'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'answer': \"Sorry, I encountered an error generating the answer.\",\n",
        "                'confidence': 0.0,\n",
        "                'method': 't5',\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def generate_answer_qa(self, question: str, context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using QA pipeline\"\"\"\n",
        "        try:\n",
        "            if not context:\n",
        "                return {\n",
        "                    'answer': \"No context available for answering.\",\n",
        "                    'confidence': 0.0,\n",
        "                    'method': 'qa_pipeline'\n",
        "                }\n",
        "\n",
        "            result = self.qa_pipeline(question=question, context=context)\n",
        "\n",
        "            return {\n",
        "                'answer': result['answer'],\n",
        "                'confidence': result['score'],\n",
        "                'method': 'qa_pipeline'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'answer': \"Sorry, I couldn't extract an answer.\",\n",
        "                'confidence': 0.0,\n",
        "                'method': 'qa_pipeline',\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def generate_answer(self, question: str, retrieved_docs: List[Dict[str, Any]],\n",
        "                       method: str = \"t5\") -> Dict[str, Any]:\n",
        "        \"\"\"Main method to generate answers\"\"\"\n",
        "        if not retrieved_docs:\n",
        "            return {\n",
        "                'answer': \"No relevant context found to answer the question.\",\n",
        "                'confidence': 0.0,\n",
        "                'sources': [],\n",
        "                'method': method\n",
        "            }\n",
        "\n",
        "        # Format context\n",
        "        context = self.format_context(retrieved_docs)\n",
        "\n",
        "        # Generate answer\n",
        "        if method == \"t5\":\n",
        "            result = self.generate_answer_t5(question, context)\n",
        "        else:\n",
        "            result = self.generate_answer_qa(question, context)\n",
        "\n",
        "        # Add metadata\n",
        "        sources = [doc.get('source', 'Unknown') for doc in retrieved_docs]\n",
        "        result.update({\n",
        "            'sources': sources,\n",
        "            'context_used': len(retrieved_docs),\n",
        "            'question': question\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n",
        "print(\"âœ… AnswerGenerator ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 6!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi2qUXs-iDlp",
        "outputId": "1d5c805d-3b25-408e-9773-25f98d158efe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… AnswerGenerator ready!\n",
            "ðŸš€ Ready to proceed to Cell 6!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"Complete RAG system combining all components\"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 300, overlap: int = 50,\n",
        "                 retrieval_model: str = \"all-MiniLM-L6-v2\",\n",
        "                 generation_model: str = \"google/flan-t5-base\"):\n",
        "\n",
        "        print(\"ðŸš€ Initializing RAG System...\")\n",
        "\n",
        "        self.config = {\n",
        "            'chunk_size': chunk_size,\n",
        "            'overlap': overlap,\n",
        "            'retrieval_model': retrieval_model,\n",
        "            'generation_model': generation_model\n",
        "        }\n",
        "\n",
        "        # Initialize components\n",
        "        self.doc_processor = DocumentProcessor(chunk_size=chunk_size, overlap=overlap)\n",
        "        self.retriever = DocumentRetriever(model_name=retrieval_model)\n",
        "        self.generator = AnswerGenerator(model_name=generation_model)\n",
        "\n",
        "        self.documents = []\n",
        "        self.is_trained = False\n",
        "\n",
        "        print(\"âœ… RAG System initialized!\")\n",
        "\n",
        "    def load_documents(self, file_paths: List[str]) -> None:\n",
        "        \"\"\"Load and process documents\"\"\"\n",
        "        print(\"ðŸ“š Loading documents into RAG system...\")\n",
        "\n",
        "        # Process documents\n",
        "        self.documents = self.doc_processor.process_documents(file_paths)\n",
        "\n",
        "        if not self.documents:\n",
        "            raise ValueError(\"No documents were processed!\")\n",
        "\n",
        "        # Build retrieval index\n",
        "        print(\"ðŸ—ï¸ Building search index...\")\n",
        "        self.retriever.build_index(self.documents)\n",
        "\n",
        "        self.is_trained = True\n",
        "        print(f\"ðŸŽ‰ RAG system ready with {len(self.documents)} chunks!\")\n",
        "\n",
        "    def answer_question(self, question: str, top_k: int = 5, method: str = \"t5\",\n",
        "                       show_sources: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"Answer a question using the RAG system\"\"\"\n",
        "        if not self.is_trained:\n",
        "            raise ValueError(\"System not trained! Load documents first.\")\n",
        "\n",
        "        if not question.strip():\n",
        "            return {\n",
        "                'answer': \"Please provide a valid question.\",\n",
        "                'confidence': 0.0,\n",
        "                'error': 'empty_question'\n",
        "            }\n",
        "\n",
        "        if show_sources:\n",
        "            print(f\"â“ Question: {question}\")\n",
        "            print(f\"ðŸ” Retrieving top {top_k} relevant documents...\")\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.retriever.retrieve(question, top_k=top_k)\n",
        "\n",
        "        if show_sources and retrieved_docs:\n",
        "            print(f\"ðŸ“„ Found {len(retrieved_docs)} relevant documents:\")\n",
        "            for i, doc in enumerate(retrieved_docs, 1):\n",
        "                score = doc.get('similarity_score', 0.0)\n",
        "                source = doc.get('source', 'Unknown')\n",
        "                print(f\"   {i}. {source} - Score: {score:.4f}\")\n",
        "\n",
        "        # Generate answer\n",
        "        if show_sources:\n",
        "            print(f\"ðŸ¤– Generating answer using {method} method...\")\n",
        "        result = self.generator.generate_answer(question, retrieved_docs, method=method)\n",
        "\n",
        "        if show_sources:\n",
        "            print(f\"âœ… Answer generated with confidence: {result['confidence']:.3f}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_system_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system statistics\"\"\"\n",
        "        if not self.documents:\n",
        "            return {\"status\": \"No documents loaded\"}\n",
        "\n",
        "        sources = {}\n",
        "        total_words = 0\n",
        "\n",
        "        for doc in self.documents:\n",
        "            source = doc.get('source', 'Unknown')\n",
        "            word_count = doc.get('word_count', 0)\n",
        "            sources[source] = sources.get(source, 0) + 1\n",
        "            total_words += word_count\n",
        "\n",
        "        return {\n",
        "            \"status\": \"Ready\" if self.is_trained else \"Not ready\",\n",
        "            \"total_chunks\": len(self.documents),\n",
        "            \"total_words\": total_words,\n",
        "            \"sources\": sources,\n",
        "            \"unique_sources\": len(sources),\n",
        "            \"avg_chunk_length\": total_words / len(self.documents) if self.documents else 0\n",
        "        }\n",
        "\n",
        "print(\"âœ… RAG System ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 7!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOftroq5iGoU",
        "outputId": "7154f42b-b400-40ff-c23a-81f9797c9abf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… RAG System ready!\n",
            "ðŸš€ Ready to proceed to Cell 7!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "def upload_documents():\n",
        "    \"\"\"Upload documents from your computer\"\"\"\n",
        "    print(\"ðŸ“ UPLOAD YOUR DOCUMENTS\")\n",
        "    print(\"=\" * 30)\n",
        "    print(\"Please upload PDF or TXT files\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    uploaded_files = []\n",
        "    for filename, file_data in uploaded.items():\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(file_data)\n",
        "        uploaded_files.append(filename)\n",
        "        print(f\"âœ… Uploaded: {filename}\")\n",
        "\n",
        "    return uploaded_files\n",
        "\n",
        "def create_sample_documents():\n",
        "    \"\"\"Create sample documents for testing\"\"\"\n",
        "    print(\"ðŸ“ Creating sample documents...\")\n",
        "\n",
        "    # Sample AI research content\n",
        "    transformer_content = \"\"\"\n",
        "Attention Is All You Need\n",
        "\n",
        "The Transformer is a neural network architecture based entirely on attention mechanisms.\n",
        "Unlike recurrent neural networks, Transformers can process sequences in parallel.\n",
        "\n",
        "Key components include:\n",
        "- Multi-head self-attention mechanism\n",
        "- Position encodings\n",
        "- Feed-forward networks\n",
        "- Layer normalization\n",
        "\n",
        "The model achieves state-of-the-art results on machine translation tasks while being more parallelizable than RNNs.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    rag_content = \"\"\"\n",
        "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
        "\n",
        "RAG combines parametric and non-parametric memory for language generation.\n",
        "The system consists of:\n",
        "- A dense passage retriever (DPR)\n",
        "- A sequence-to-sequence generator (BART)\n",
        "\n",
        "RAG models can generate more factual and specific responses compared to parametric-only models.\n",
        "The approach allows updating knowledge without retraining by modifying the retrieval corpus.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    # Save sample files\n",
        "    with open('transformer_paper.txt', 'w') as f:\n",
        "        f.write(transformer_content)\n",
        "\n",
        "    with open('rag_paper.txt', 'w') as f:\n",
        "        f.write(rag_content)\n",
        "\n",
        "    print(\"âœ… Created sample documents:\")\n",
        "    print(\"   - transformer_paper.txt\")\n",
        "    print(\"   - rag_paper.txt\")\n",
        "\n",
        "    return ['transformer_paper.txt', 'rag_paper.txt']\n",
        "\n",
        "print(\"âœ… File upload functions ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 8!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muwCF0pyiKVU",
        "outputId": "105af36e-acd8-4a5b-e40e-8128101d443e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File upload functions ready!\n",
            "ðŸš€ Ready to proceed to Cell 8!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def quick_setup():\n",
        "    \"\"\"Quick setup with default settings\"\"\"\n",
        "    print(\"âš¡ QUICK RAG SETUP\")\n",
        "    print(\"=\" * 20)\n",
        "\n",
        "    # Create sample documents\n",
        "    file_paths = create_sample_documents()\n",
        "\n",
        "    # Initialize RAG system\n",
        "    rag_system = RAGSystem(\n",
        "        chunk_size=300,\n",
        "        retrieval_model='all-MiniLM-L6-v2',\n",
        "        generation_model='google/flan-t5-base'\n",
        "    )\n",
        "\n",
        "    # Load documents\n",
        "    rag_system.load_documents(file_paths)\n",
        "\n",
        "    print(\"ðŸŽ‰ RAG system ready!\")\n",
        "    return rag_system\n",
        "\n",
        "def setup_with_upload():\n",
        "    \"\"\"Setup with your own uploaded files\"\"\"\n",
        "    print(\"ðŸ“ SETUP WITH YOUR FILES\")\n",
        "    print(\"=\" * 25)\n",
        "\n",
        "    # Upload files\n",
        "    file_paths = upload_documents()\n",
        "\n",
        "    if not file_paths:\n",
        "        print(\"âš ï¸ No files uploaded, using samples...\")\n",
        "        file_paths = create_sample_documents()\n",
        "\n",
        "    # Initialize RAG system\n",
        "    rag_system = RAGSystem()\n",
        "\n",
        "    # Load documents\n",
        "    rag_system.load_documents(file_paths)\n",
        "\n",
        "    return rag_system\n",
        "\n",
        "print(\"âœ… Setup functions ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 9!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtgJwnpsiOBL",
        "outputId": "f9e062b6-550c-4cc6-af44-efd78b428a29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup functions ready!\n",
            "ðŸš€ Ready to proceed to Cell 9!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag_system(rag_system):\n",
        "    \"\"\"Test the RAG system with sample questions\"\"\"\n",
        "    print(\"ðŸ§ª TESTING RAG SYSTEM\")\n",
        "    print(\"=\" * 25)\n",
        "\n",
        "    test_questions = [\n",
        "        \"What is the main innovation of the Transformer?\",\n",
        "        \"How does RAG work?\",\n",
        "        \"What are the key components mentioned?\"\n",
        "    ]\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\n[{i}] Testing: {question}\")\n",
        "\n",
        "        try:\n",
        "            result = rag_system.answer_question(question, show_sources=False)\n",
        "            print(f\"âœ… Answer: {result['answer'][:100]}...\")\n",
        "            print(f\"ðŸ“Š Confidence: {result['confidence']:.3f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Testing complete!\")\n",
        "\n",
        "def system_status(rag_system):\n",
        "    \"\"\"Check system status\"\"\"\n",
        "    print(\"ðŸ“Š SYSTEM STATUS\")\n",
        "    print(\"=\" * 20)\n",
        "\n",
        "    if not rag_system.is_trained:\n",
        "        print(\"âŒ System not ready\")\n",
        "        return\n",
        "\n",
        "    stats = rag_system.get_system_stats()\n",
        "    print(f\"âœ… Status: {stats['status']}\")\n",
        "    print(f\"ðŸ“„ Document chunks: {stats['total_chunks']}\")\n",
        "    print(f\"ðŸ“š Sources: {stats['unique_sources']}\")\n",
        "    print(f\"ðŸ“ Total words: {stats['total_words']:,}\")\n",
        "\n",
        "print(\"âœ… Testing functions ready!\")\n",
        "print(\"ðŸš€ Ready to proceed to Cell 10!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rbDSrB2iRRn",
        "outputId": "cb589385-6901-4cdb-f5b1-5bacfc772a5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Testing functions ready!\n",
            "ðŸš€ Ready to proceed to Cell 10!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"Main function to start the RAG system\"\"\"\n",
        "    print(\"ðŸŽ¯ RAG SYSTEM FOR AI RESEARCH PAPERS\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Choose your option:\")\n",
        "    print(\"1. âš¡ Quick setup (sample documents)\")\n",
        "    print(\"2. ðŸ“ Upload your own files\")\n",
        "    print(\"3. ðŸ“Š Check system status\")\n",
        "\n",
        "    choice = input(\"\\nEnter choice (1-3): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        print(\"\\nâš¡ Starting quick setup...\")\n",
        "        rag_system = quick_setup()\n",
        "        return rag_system\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        print(\"\\nðŸ“ Starting setup with file upload...\")\n",
        "        rag_system = setup_with_upload()\n",
        "        return rag_system\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        print(\"ðŸ“Š System status check...\")\n",
        "        # This would check existing system\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ Invalid choice, using quick setup...\")\n",
        "        rag_system = quick_setup()\n",
        "        return rag_system\n",
        "\n",
        "# Ready to use!\n",
        "print(\"ðŸŽ¯ RAG SYSTEM COMPLETE!\")\n",
        "print(\"=\" * 30)\n",
        "print(\"ðŸš€ To start: rag_system = main()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfIMpHjxiUQs",
        "outputId": "2ba358b0-8a07-4fc5-d88a-9ee3094b8990"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ RAG SYSTEM COMPLETE!\n",
            "==============================\n",
            "ðŸš€ To start: rag_system = main()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question_simple(rag_system):\n",
        "    \"\"\"Simple interactive question-answering interface\"\"\"\n",
        "    print(\"\\nðŸ¤– RAG QUESTION ANSWERING SYSTEM\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Ask me questions about your documents!\")\n",
        "    print(\"Type 'quit', 'exit', or 'stop' to end the session.\")\n",
        "    print(\"Type 'help' for commands.\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Check if system is ready\n",
        "    if not rag_system.is_trained:\n",
        "        print(\"âŒ RAG system not ready! Please load documents first.\")\n",
        "        return\n",
        "\n",
        "    # Show system status\n",
        "    stats = rag_system.get_system_stats()\n",
        "    print(f\"ðŸ“Š System loaded with {stats['total_chunks']} chunks from {stats['unique_sources']} documents\")\n",
        "    print()\n",
        "\n",
        "    question_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Get user input\n",
        "            question = input(\"â“ Your question: \").strip()\n",
        "\n",
        "            # Handle special commands\n",
        "            if question.lower() in ['quit', 'exit', 'stop', 'q']:\n",
        "                print(\"ðŸ‘‹ Thanks for using the RAG system! Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if question.lower() == 'help':\n",
        "                print(\"\\nðŸ“‹ Available commands:\")\n",
        "                print(\"  - Type your question to get an answer\")\n",
        "                print(\"  - 'status' - Show system information\")\n",
        "                print(\"  - 'examples' - Show example questions\")\n",
        "                print(\"  - 'clear' - Clear screen\")\n",
        "                print(\"  - 'quit'/'exit'/'stop' - End session\")\n",
        "                print()\n",
        "                continue\n",
        "\n",
        "            if question.lower() == 'status':\n",
        "                print(f\"\\nðŸ“Š System Status:\")\n",
        "                print(f\"   Documents: {stats['unique_sources']}\")\n",
        "                print(f\"   Chunks: {stats['total_chunks']}\")\n",
        "                print(f\"   Total words: {stats['total_words']:,}\")\n",
        "                print(f\"   Questions answered: {question_count}\")\n",
        "                print()\n",
        "                continue\n",
        "\n",
        "            if question.lower() == 'examples':\n",
        "                print(\"\\nðŸ’¡ Example questions you can ask:\")\n",
        "                print(\"   - What are the main components of the Transformer?\")\n",
        "                print(\"   - How does attention work?\")\n",
        "                print(\"   - What is retrieval-augmented generation?\")\n",
        "                print(\"   - Explain the architecture described in the papers\")\n",
        "                print()\n",
        "                continue\n",
        "\n",
        "            if question.lower() == 'clear':\n",
        "                import os\n",
        "                os.system('clear' if os.name == 'posix' else 'cls')\n",
        "                continue\n",
        "\n",
        "            if not question:\n",
        "                print(\"âš ï¸ Please enter a question.\")\n",
        "                continue\n",
        "\n",
        "            # Process the question\n",
        "            print(f\"\\nðŸ” Processing your question...\")\n",
        "\n",
        "            try:\n",
        "                # Get answer from RAG system\n",
        "                result = rag_system.answer_question(\n",
        "                    question,\n",
        "                    top_k=5,\n",
        "                    method=\"t5\",\n",
        "                    show_sources=False\n",
        "                )\n",
        "\n",
        "                question_count += 1\n",
        "\n",
        "                # Display results\n",
        "                print(f\"\\nðŸ¤– Answer:\")\n",
        "                print(f\"   {result['answer']}\")\n",
        "                print(f\"\\nðŸ“Š Confidence: {result['confidence']:.3f}\")\n",
        "\n",
        "                # Show sources\n",
        "                if 'sources' in result and result['sources']:\n",
        "                    print(f\"ðŸ“š Sources: {', '.join(result['sources'])}\")\n",
        "\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error processing question: {e}\")\n",
        "                print(\"Please try again with a different question.\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nðŸ‘‹ Session interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Unexpected error: {e}\")\n",
        "            print(\"Please try again.\")\n",
        "\n",
        "def ask_question_advanced(rag_system):\n",
        "    \"\"\"Advanced interactive interface with more options\"\"\"\n",
        "    print(\"\\nðŸš€ ADVANCED RAG QUESTION ANSWERING\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    if not rag_system.is_trained:\n",
        "        print(\"âŒ RAG system not ready! Please load documents first.\")\n",
        "        return\n",
        "\n",
        "    # Configuration options\n",
        "    config = {\n",
        "        'top_k': 5,\n",
        "        'method': 't5',\n",
        "        'show_sources': True,\n",
        "        'show_retrieved_docs': False\n",
        "    }\n",
        "\n",
        "    print(\"âš™ï¸ Current settings:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "    print()\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            print(\"ðŸŽ¯ Options:\")\n",
        "            print(\"1. Ask a question\")\n",
        "            print(\"2. Change settings\")\n",
        "            print(\"3. Show system stats\")\n",
        "            print(\"4. Exit\")\n",
        "\n",
        "            choice = input(\"\\nSelect option (1-4): \").strip()\n",
        "\n",
        "            if choice == '1':\n",
        "                question = input(\"\\nâ“ Your question: \").strip()\n",
        "\n",
        "                if not question:\n",
        "                    print(\"âš ï¸ Please enter a question.\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nðŸ” Processing with settings: {config}\")\n",
        "\n",
        "                try:\n",
        "                    result = rag_system.answer_question(\n",
        "                        question,\n",
        "                        top_k=config['top_k'],\n",
        "                        method=config['method'],\n",
        "                        show_sources=config['show_sources']\n",
        "                    )\n",
        "\n",
        "                    print(f\"\\nðŸ¤– Answer:\")\n",
        "                    print(f\"   {result['answer']}\")\n",
        "                    print(f\"\\nðŸ“Š Confidence: {result['confidence']:.3f}\")\n",
        "\n",
        "                    if config['show_retrieved_docs'] and 'sources' in result:\n",
        "                        print(f\"ðŸ“š Retrieved from: {', '.join(result['sources'])}\")\n",
        "\n",
        "                    # Ask for feedback\n",
        "                    feedback = input(\"\\nðŸ‘ Was this answer helpful? (y/n): \").strip().lower()\n",
        "                    if feedback in ['n', 'no']:\n",
        "                        suggestion = input(\"ðŸ’¡ How could the answer be improved? \")\n",
        "                        print(f\"ðŸ“ Feedback noted: {suggestion}\")\n",
        "\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"âŒ Error: {e}\")\n",
        "\n",
        "            elif choice == '2':\n",
        "                print(\"\\nâš™ï¸ Change settings:\")\n",
        "                print(\"1. Number of documents to retrieve (top_k)\")\n",
        "                print(\"2. Generation method (t5/qa_pipeline)\")\n",
        "                print(\"3. Show sources (True/False)\")\n",
        "                print(\"4. Show retrieved documents (True/False)\")\n",
        "\n",
        "                setting_choice = input(\"Select setting to change (1-4): \").strip()\n",
        "\n",
        "                if setting_choice == '1':\n",
        "                    try:\n",
        "                        new_k = int(input(f\"Current top_k: {config['top_k']}. New value: \"))\n",
        "                        config['top_k'] = max(1, min(50, new_k))\n",
        "                        print(f\"âœ… Updated top_k to {config['top_k']}\")\n",
        "                    except ValueError:\n",
        "                        print(\"âŒ Invalid number.\")\n",
        "\n",
        "                elif setting_choice == '2':\n",
        "                    method = input(\"Method (t5/qa_pipeline): \").strip().lower()\n",
        "                    if method in ['t5', 'qa_pipeline']:\n",
        "                        config['method'] = method\n",
        "                        print(f\"âœ… Updated method to {method}\")\n",
        "                    else:\n",
        "                        print(\"âŒ Invalid method. Use 't5' or 'qa_pipeline'\")\n",
        "\n",
        "                elif setting_choice == '3':\n",
        "                    show = input(\"Show sources (y/n): \").strip().lower()\n",
        "                    config['show_sources'] = show in ['y', 'yes', 'true']\n",
        "                    print(f\"âœ… Updated show_sources to {config['show_sources']}\")\n",
        "\n",
        "                elif setting_choice == '4':\n",
        "                    show = input(\"Show retrieved documents (y/n): \").strip().lower()\n",
        "                    config['show_retrieved_docs'] = show in ['y', 'yes', 'true']\n",
        "                    print(f\"âœ… Updated show_retrieved_docs to {config['show_retrieved_docs']}\")\n",
        "\n",
        "            elif choice == '3':\n",
        "                stats = rag_system.get_system_stats()\n",
        "                print(f\"\\nðŸ“Š System Statistics:\")\n",
        "                for key, value in stats.items():\n",
        "                    if isinstance(value, dict):\n",
        "                        print(f\"   {key}:\")\n",
        "                        for k, v in value.items():\n",
        "                            print(f\"     {k}: {v}\")\n",
        "                    else:\n",
        "                        print(f\"   {key}: {value}\")\n",
        "\n",
        "            elif choice == '4':\n",
        "                print(\"ðŸ‘‹ Goodbye!\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"âŒ Invalid option. Please select 1-4.\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nðŸ‘‹ Session interrupted. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "def batch_question_answering(rag_system, questions_file=None):\n",
        "    \"\"\"Process multiple questions from a file or predefined list\"\"\"\n",
        "    print(\"ðŸ“‹ BATCH QUESTION ANSWERING\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    if not rag_system.is_trained:\n",
        "        print(\"âŒ RAG system not ready!\")\n",
        "        return\n",
        "\n",
        "    # Default questions if no file provided\n",
        "    default_questions = [\n",
        "        \"What is the main innovation of the Transformer architecture?\",\n",
        "        \"How does the attention mechanism work?\",\n",
        "        \"What are the key components of a Transformer?\",\n",
        "        \"How does RAG combine parametric and non-parametric memory?\",\n",
        "        \"What are the advantages of self-attention over recurrent layers?\",\n",
        "        \"How does retrieval-augmented generation work?\",\n",
        "        \"What is the difference between RAG-Token and RAG-Sequence?\",\n",
        "        \"What are the applications of the Transformer model?\"\n",
        "    ]\n",
        "\n",
        "    questions = default_questions\n",
        "\n",
        "    if questions_file:\n",
        "        try:\n",
        "            with open(questions_file, 'r') as f:\n",
        "                questions = [line.strip() for line in f if line.strip()]\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âš ï¸ File {questions_file} not found. Using default questions.\")\n",
        "\n",
        "    print(f\"ðŸ” Processing {len(questions)} questions...\")\n",
        "    print()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"[{i}/{len(questions)}] {question}\")\n",
        "\n",
        "        try:\n",
        "            result = rag_system.answer_question(question, show_sources=False)\n",
        "            results.append({\n",
        "                'question': question,\n",
        "                'answer': result['answer'],\n",
        "                'confidence': result['confidence']\n",
        "            })\n",
        "\n",
        "            print(f\"âœ… Answer: {result['answer'][:100]}...\")\n",
        "            print(f\"ðŸ“Š Confidence: {result['confidence']:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "            results.append({\n",
        "                'question': question,\n",
        "                'answer': f\"Error: {e}\",\n",
        "                'confidence': 0.0\n",
        "            })\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Summary\n",
        "    avg_confidence = sum(r['confidence'] for r in results) / len(results)\n",
        "    print(f\"\\nðŸ“ˆ Summary:\")\n",
        "    print(f\"   Questions processed: {len(results)}\")\n",
        "    print(f\"   Average confidence: {avg_confidence:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def save_qa_session(results, filename=\"qa_session.txt\"):\n",
        "    \"\"\"Save Q&A results to a file\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(\"RAG Question-Answering Session Results\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "            for i, result in enumerate(results, 1):\n",
        "                f.write(f\"Question {i}: {result['question']}\\n\")\n",
        "                f.write(f\"Answer: {result['answer']}\\n\")\n",
        "                f.write(f\"Confidence: {result['confidence']:.3f}\\n\")\n",
        "                f.write(\"-\" * 30 + \"\\n\\n\")\n",
        "\n",
        "        print(f\"âœ… Results saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error saving results: {e}\")\n",
        "\n",
        "print(\"âœ… Interactive interfaces ready!\")\n",
        "print(\"ðŸš€ Available functions:\")\n",
        "print(\"   - ask_question_simple(rag_system) - Basic Q&A interface\")\n",
        "print(\"   - ask_question_advanced(rag_system) - Advanced interface with settings\")\n",
        "print(\"   - batch_question_answering(rag_system) - Process multiple questions\")\n",
        "print(\"   - save_qa_session(results) - Save Q&A results to file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6bTkn0eiWy-",
        "outputId": "5980d342-052e-4d6b-ee32-94b39596b6d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Interactive interfaces ready!\n",
            "ðŸš€ Available functions:\n",
            "   - ask_question_simple(rag_system) - Basic Q&A interface\n",
            "   - ask_question_advanced(rag_system) - Advanced interface with settings\n",
            "   - batch_question_answering(rag_system) - Process multiple questions\n",
            "   - save_qa_session(results) - Save Q&A results to file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_rag_system():\n",
        "    \"\"\"Complete workflow: Setup system and start Q&A\"\"\"\n",
        "    print(\"ðŸš€ STARTING COMPLETE RAG SYSTEM\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Step 1: Initialize the system\n",
        "    print(\"Step 1: Setting up RAG system...\")\n",
        "    rag_system = main()\n",
        "\n",
        "    if rag_system is None:\n",
        "        print(\"âŒ System setup failed!\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Test the system\n",
        "    print(\"\\nStep 2: Testing the system...\")\n",
        "    test_rag_system(rag_system)\n",
        "\n",
        "    # Step 3: Show system status\n",
        "    print(\"\\nStep 3: System status...\")\n",
        "    system_status(rag_system)\n",
        "\n",
        "    # Step 4: Start interactive Q&A\n",
        "    print(\"\\nStep 4: Starting interactive question-answering...\")\n",
        "\n",
        "    interface_choice = input(\"\\nChoose interface:\\n1. Simple Q&A\\n2. Advanced Q&A\\n3. Batch processing\\nEnter choice (1-3): \").strip()\n",
        "\n",
        "    if interface_choice == \"1\":\n",
        "        ask_question_simple(rag_system)\n",
        "    elif interface_choice == \"2\":\n",
        "        ask_question_advanced(rag_system)\n",
        "    elif interface_choice == \"3\":\n",
        "        results = batch_question_answering(rag_system)\n",
        "        save_choice = input(\"\\nSave results to file? (y/n): \").strip().lower()\n",
        "        if save_choice in ['y', 'yes']:\n",
        "            filename = input(\"Enter filename (default: qa_session.txt): \").strip()\n",
        "            if not filename:\n",
        "                filename = \"qa_session.txt\"\n",
        "            save_qa_session(results, filename)\n",
        "    else:\n",
        "        print(\"Starting simple interface by default...\")\n",
        "        ask_question_simple(rag_system)\n",
        "\n",
        "    return rag_system"
      ],
      "metadata": {
        "id": "xEhQ7pNiiaP2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸŽ¯ COMPLETE RAG SYSTEM READY!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"ðŸ“‹ Instructions:\")\n",
        "print(\"1. Run all cells in order (1-12)\")\n",
        "print(\"2. Execute: run_complete_rag_system()\")\n",
        "print(\"3. Follow the prompts to set up and use the system\")\n",
        "print(\"4. Ask questions using the interactive interface\")\n",
        "print()\n",
        "print(\"ðŸš€ To start the complete system:\")\n",
        "print(\"   run_complete_rag_system()\")\n",
        "print()\n",
        "print(\"ðŸŽ¯ Or run individual components:\")\n",
        "print(\"   rag_system = main()                    # Setup system\")\n",
        "print(\"   ask_question_simple(rag_system)       # Start Q&A\")\n",
        "\n",
        "# Execute the complete system\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment the line below to auto-run the system\n",
        "    # run_complete_rag_system()\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoKN9zoPifgd",
        "outputId": "654e8667-977a-4a08-d343-068d1be008d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ COMPLETE RAG SYSTEM READY!\n",
            "==================================================\n",
            "ðŸ“‹ Instructions:\n",
            "1. Run all cells in order (1-12)\n",
            "2. Execute: run_complete_rag_system()\n",
            "3. Follow the prompts to set up and use the system\n",
            "4. Ask questions using the interactive interface\n",
            "\n",
            "ðŸš€ To start the complete system:\n",
            "   run_complete_rag_system()\n",
            "\n",
            "ðŸŽ¯ Or run individual components:\n",
            "   rag_system = main()                    # Setup system\n",
            "   ask_question_simple(rag_system)       # Start Q&A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup system\n",
        "rag_system = main()\n",
        "\n",
        "# Start asking questions\n",
        "ask_question_simple(rag_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1ca2756efb594899b22bc3f30a4ebbbc",
            "b33b09453ddb47be92a3f1259670e1cd",
            "41b2b0d4f85942f8a5df585372828437",
            "c2cc0472befe4a6194bb7ecded8f86ce",
            "9c8f8486497746b19bab28b9be40de28",
            "3734c6598fd445c9875120e0a1d434d1",
            "4f3f74330eed45b9816312ae82b4e3e3",
            "ca7b145a69754a15949f68b20911e67b",
            "053bedc6a395443882fd27cf975b7d1d",
            "72f82a27d9104048b09c584ca08e472b",
            "e0a356704f64442c87dc40989176824d"
          ]
        },
        "id": "Y-nJureNjaYN",
        "outputId": "d10d87a6-f926-43e6-82ac-86284987fb9d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ RAG SYSTEM FOR AI RESEARCH PAPERS\n",
            "========================================\n",
            "Choose your option:\n",
            "1. âš¡ Quick setup (sample documents)\n",
            "2. ðŸ“ Upload your own files\n",
            "3. ðŸ“Š Check system status\n",
            "\n",
            "Enter choice (1-3): 1\n",
            "\n",
            "âš¡ Starting quick setup...\n",
            "âš¡ QUICK RAG SETUP\n",
            "====================\n",
            "ðŸ“ Creating sample documents...\n",
            "âœ… Created sample documents:\n",
            "   - transformer_paper.txt\n",
            "   - rag_paper.txt\n",
            "ðŸš€ Initializing RAG System...\n",
            "ðŸ“„ DocumentProcessor ready: 300 words per chunk\n",
            "ðŸ” DocumentRetriever ready with all-MiniLM-L6-v2\n",
            "ðŸ¤– Loading answer generation model: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Answer generator ready!\n",
            "âœ… RAG System initialized!\n",
            "ðŸ“š Loading documents into RAG system...\n",
            "ðŸ“š Processing 2 documents...\n",
            "ðŸ“„ Processing: transformer_paper.txt\n",
            "   âœ… Created 1 chunks\n",
            "ðŸ“„ Processing: rag_paper.txt\n",
            "   âœ… Created 1 chunks\n",
            "ðŸŽ‰ Total chunks: 2\n",
            "ðŸ—ï¸ Building search index...\n",
            "ðŸ—ï¸ Building search index for 2 chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ca2756efb594899b22bc3f30a4ebbbc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Search index ready! Dimension: 384\n",
            "ðŸŽ‰ RAG system ready with 2 chunks!\n",
            "ðŸŽ‰ RAG system ready!\n",
            "\n",
            "ðŸ¤– RAG QUESTION ANSWERING SYSTEM\n",
            "========================================\n",
            "Ask me questions about your documents!\n",
            "Type 'quit', 'exit', or 'stop' to end the session.\n",
            "Type 'help' for commands.\n",
            "========================================\n",
            "ðŸ“Š System loaded with 2 chunks from 2 documents\n",
            "\n",
            "â“ Your question: what is this\n",
            "\n",
            "ðŸ” Processing your question...\n",
            "\n",
            "ðŸ¤– Answer:\n",
            "   Transformer is a neural network architecture\n",
            "\n",
            "ðŸ“Š Confidence: 0.800\n",
            "ðŸ“š Sources: rag_paper.txt, transformer_paper.txt\n",
            "--------------------------------------------------\n",
            "â“ Your question: exit\n",
            "ðŸ‘‹ Thanks for using the RAG system! Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_system = main()\n",
        "\n",
        "# Start asking questions\n",
        "ask_question_simple(rag_system)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "124d30b57cc24b539389ddb9d88a8ad0",
            "6f2f47ebb5bc403c9d47970db12d5006",
            "8263ffcb89d5429fa4aea54e09a667fc",
            "7783212b9a1d492e9d5eca44fa73e580",
            "7c1aa37406f340a483eb8688df1175e3",
            "5b0a8cf907b34073a265c06c0061fdd2",
            "a4fb31be67b643c9b875d3c8375357c5",
            "08d7216aa55943dc806b2717f64a24a4",
            "c440cffc81cc4cc0bb4a498e8b848a7e",
            "689f4bb0f17044ca8e8bce7c636fe0cc",
            "ec839ca3980a40b48abd5b8737ab3021"
          ]
        },
        "id": "ePeH_y1fjceo",
        "outputId": "d9d6a747-43db-44c5-c1aa-dc0d1f4585ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ RAG SYSTEM FOR AI RESEARCH PAPERS\n",
            "========================================\n",
            "Choose your option:\n",
            "1. âš¡ Quick setup (sample documents)\n",
            "2. ðŸ“ Upload your own files\n",
            "3. ðŸ“Š Check system status\n",
            "\n",
            "Enter choice (1-3): 2\n",
            "\n",
            "ðŸ“ Starting setup with file upload...\n",
            "ðŸ“ SETUP WITH YOUR FILES\n",
            "=========================\n",
            "ðŸ“ UPLOAD YOUR DOCUMENTS\n",
            "==============================\n",
            "Please upload PDF or TXT files\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4716079-45b7-4bdd-bcc4-0347979337c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f4716079-45b7-4bdd-bcc4-0347979337c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1706.03762v7.pdf to 1706.03762v7.pdf\n",
            "Saving 2005.11401v4.pdf to 2005.11401v4.pdf\n",
            "Saving 2005.14165v4.pdf to 2005.14165v4.pdf\n",
            "âœ… Uploaded: 1706.03762v7.pdf\n",
            "âœ… Uploaded: 2005.11401v4.pdf\n",
            "âœ… Uploaded: 2005.14165v4.pdf\n",
            "ðŸš€ Initializing RAG System...\n",
            "ðŸ“„ DocumentProcessor ready: 300 words per chunk\n",
            "ðŸ” DocumentRetriever ready with all-MiniLM-L6-v2\n",
            "ðŸ¤– Loading answer generation model: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Answer generator ready!\n",
            "âœ… RAG System initialized!\n",
            "ðŸ“š Loading documents into RAG system...\n",
            "ðŸ“š Processing 3 documents...\n",
            "ðŸ“„ Processing: 1706.03762v7.pdf\n",
            "   âœ… Created 22 chunks\n",
            "ðŸ“„ Processing: 2005.11401v4.pdf\n",
            "   âœ… Created 37 chunks\n",
            "ðŸ“„ Processing: 2005.14165v4.pdf\n",
            "   âœ… Created 140 chunks\n",
            "ðŸŽ‰ Total chunks: 199\n",
            "ðŸ—ï¸ Building search index...\n",
            "ðŸ—ï¸ Building search index for 199 chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "124d30b57cc24b539389ddb9d88a8ad0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Search index ready! Dimension: 384\n",
            "ðŸŽ‰ RAG system ready with 199 chunks!\n",
            "\n",
            "ðŸ¤– RAG QUESTION ANSWERING SYSTEM\n",
            "========================================\n",
            "Ask me questions about your documents!\n",
            "Type 'quit', 'exit', or 'stop' to end the session.\n",
            "Type 'help' for commands.\n",
            "========================================\n",
            "ðŸ“Š System loaded with 199 chunks from 3 documents\n",
            "\n",
            "â“ Your question: what is this\n",
            "\n",
            "ðŸ” Processing your question...\n",
            "\n",
            "ðŸ¤– Answer:\n",
            "   Wikipedia.\n",
            "\n",
            "ðŸ“Š Confidence: 0.800\n",
            "ðŸ“š Sources: 2005.14165v4.pdf, 2005.14165v4.pdf, 2005.11401v4.pdf, 2005.11401v4.pdf, 2005.14165v4.pdf\n",
            "--------------------------------------------------\n",
            "â“ Your question: exit\n",
            "ðŸ‘‹ Thanks for using the RAG system! Goodbye!\n"
          ]
        }
      ]
    }
  ]
}